
原理：

    存在一个样本数据集，也称作训练样本集，并且样本中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与样本集中的数据对应的特征进行比较，然后算法提取样本集中特征最相似的数据（最近邻）的分类标签。一般来说，我们只选择样本集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数，最后，选择k个最相似的数据中出现次数最多的分类，作为新数据的分类。

    测试过程中，计算测试样本与每个训练样本的距离，选取与测试样本距离最近的前k个训练样本。然后对着k个训练样本的label进行投票，票数最多的那一类别即为测试样本所归类。

    kNN算法非常简单，可以说在训练过程中基本没有算法参与，只有存储训练样本。可以说KNN算法实际上是一种识记类算法。

优点：

    精度高，对异常数据不敏感（你的类别是由邻居中的大多数决定的，一个异常邻居并不能影响太大），无数据输入假定；算法简单，容易理解，无复杂机器学习算法。

缺点：

    整个训练过程需要将所有的训练样本及其输出label存储起来，因此，空间成本很大。

    测试过程中，每个测试样本都需要与所有的训练样本进行比较，运行时间成本很大。

    采用距离比较的方式，分类准确率不高。


```python

```
